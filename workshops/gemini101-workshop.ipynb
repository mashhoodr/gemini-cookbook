{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini BuildWithAI Workshop\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mashhoodr/gemini-cookbook/blob/main/workshops/gemini101-workshop.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "\n",
    "\n",
    "This notebook is designed to run you through different features of Google Gemini. Please follow the instructions of the trainer. It has content taken from different cookbook files, aggregated for convenience. \n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "The objective of this workshop is to help the attendees become familiar with the offerings of Google Gemini, and give them an opportunity to try out the API themselves. We run through a few exercises to help understand the use cases for the different functionalities present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Gemini API uses API keys for authentication. We will now setup the API key in this colab - and test out our authentication. Your trainer has already demoed the instructions below.\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.  \n",
    "\n",
    "Remember to treat your API key like a password. Do not accidentally save it in a notebook or source file you later commit to GitHub. This notebook shows you two ways you can securely store your API key.\n",
    "\n",
    "* If you are using Google Colab, we recommend you store your key in Colab Secrets.\n",
    "\n",
    "* If you are using a different development environment (or calling the Gemini API through `cURL` in your terminal), we recommend you store your key in an environment variable.\n",
    "\n",
    "Let's start with Colab Secrets.\n",
    "\n",
    "Add your API key to the Colab Secrets manager to securely store it.\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ **Secrets** tab in the left panel.\n",
    "   \n",
    "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
    "\n",
    "2. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "3. Copy/paste your API key into the `Value` input box of `GOOGLE_API_KEY`.\n",
    "4. Toggle the button on the left to allow notebook access to the secret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the SDK with your API key.\n",
    "\n",
    "You'll call `genai.configure` with your API key, but instead of pasting your key into the notebook, you'll read it from Colab Secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "from google.colab import userdata\n",
    "from pydantic import BaseModel\n",
    "\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now you're ready to use the Gemini API.\n",
    "\n",
    "Now lets list our all the models we have available to use, before we continue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)\n",
    "\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running your first prompt\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpro = genai.GenerativeModel('gemini-1.5-pro')\n",
    "response = model_gpro.generate_content(\"Write a short poem on Python programming language.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Do it yourself: Update the above using the latest Gemini version available*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use images in your prompt\n",
    "\n",
    "Here we download an image from a URL and pass that image in our prompt.\n",
    "\n",
    "First, we download the image and load it with PIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o image.jpg https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "img = PIL.Image.open('image.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"This image contains a sketch of a potential product along with some notes.\n",
    "Given the product sketch, describe the product as thoroughly as possible based on what you\n",
    "see in the image, making sure to note all of the product features. Return output in json format:\n",
    "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can include the image in our prompt by just passing a list of items to `generate_content`. Note that you will need to use the `gemini-pro-vision` model if your prompt contains images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_gpro.generate_content([prompt, img])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Prompt Engineering\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "We have shared several important examples below, if you are interested in diving into more details, please checkout: https://www.promptingguide.ai/\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt.\n",
    "e.g. \n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety\n",
    "\n",
    "Creating good prompts needs some thought and structure, the following points should be considered when generating a good prompt.\n",
    "\n",
    "1. Define the task to perform. e.g. Summarize this text.\n",
    "2. Specify any constraints e.g. Summarize this text in two sentences.\n",
    "3. Define the format of the response e.g. Summarize this text as bullets points of key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different prompt instructions from above.\n",
    "prompt = \"\"\"\n",
    "Summarize this text as bullets points of key information.\n",
    "Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\n",
    "faster than any modern traditional computer. At very tiny scales, physical matter acts as both\n",
    "particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\n",
    "The operating principles of quantum devices is beyond the scope of classical physics. When deployed\n",
    "at scale, quantum computers could be used in a wide variety of applications such as: in\n",
    "cybersecurity to break existing encryption methods while helping researchers create new ones, in\n",
    "meteorology to develop better weather forecasting etc. However, the current state of the art quantum\n",
    "computers are still largely experimental and impractical.\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Do it yourself: Try summarizing the above paragraph with a simpler explaination (ELI5)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Include few-shot examples. \n",
    "\n",
    "You can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called few-shot prompts, while prompts that provide no examples are called zero-shot prompts. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Instructions: Tell me the subject that each lesson topic belongs to.\n",
    "\n",
    "Lesson Topic: The Life Cycle of a Butterfly -> Subject: Science\n",
    "Lesson Topic: Using Commas -> Subject: Language Arts (Grammar)\n",
    "Lesson Topic: Solving Equations with X -> Subject: Math\n",
    "Your Turn:\n",
    "Lesson Topic: The Different Parts of Speech -> Subject: _____\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Prompt the model to format its response \n",
    "\n",
    "To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of 10 countries with their capitals in the following json format:\n",
    "{country: country_name, capital: capital_name}\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply a schema through model configuration\n",
    "The following example does the following:\n",
    "\n",
    "Instantiates a model configured through a schema to respond with JSON.\n",
    "Prompts the model to return cookie recipes.\n",
    "\n",
    "The Gemini API Python client library supports schemas defined with the following types (where AllowedType is any allowed type):\n",
    "\n",
    "- int\n",
    "- float\n",
    "- bool\n",
    "- str\n",
    "- list[AllowedType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BaseModel):\n",
    "  recipe_name: str\n",
    "  ingredients: list[str]\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents='List a few popular cookie recipes.',\n",
    "    config={\n",
    "        'response_mime_type': 'application/json',\n",
    "        'response_schema': list[Recipe],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Use the response as a JSON string.\n",
    "print(response.text)\n",
    "\n",
    "# Use instantiated objects.\n",
    "my_recipes: list[Recipe] = response.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use an enum to constrain output\n",
    "In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an enum in your schema. You can use an enum option anywhere you could use a str in the response_schema, because an enum is a list of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.\n",
    "\n",
    "For example, assume that you're developing an application to classify musical instruments into one of five categories: \"Percussion\", \"String\", \"Woodwind\", \"Brass\", or \"\"Keyboard\"\". You could create an enum to help with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Instrument(enum.Enum):\n",
    "  PERCUSSION = \"Percussion\"\n",
    "  STRING = \"String\"\n",
    "  WOODWIND = \"Woodwind\"\n",
    "  BRASS = \"Brass\"\n",
    "  KEYBOARD = \"Keyboard\"\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents='What type of instrument is an oboe?',\n",
    "    config={\n",
    "        'response_mime_type': 'text/x.enum',\n",
    "        'response_schema': Instrument,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch out for hallucinations!\n",
    "\n",
    "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isnâ€™t a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### _Do it yourself._\n",
    "`10 mins`\n",
    "\n",
    "1. Generate some Python tips for a newsletter. How can you make a good prompt to deliver unique tips on multiple attempts?\n",
    "2. Use the following image, ask Gemin to solve the puzzle and explain it step by step. https://i.ibb.co/68ww1v8/Screenshot-2024-04-12-at-4-57-14-PM.png\n",
    "3. Generate a SQL query using Gemini, from a table `Countries`, with columns `CountryName` and `CapitalName`, to select all those countries whose capital starts with `M`. \n",
    "4. Write a script to categories the following topics into subjects. The response should use the enum from the subject list below, and should respond with a fixed structure of: \n",
    "\n",
    "{ topic: string, subject: string }\n",
    "\n",
    "Topics:\n",
    "- Comprehension\n",
    "- Communication\n",
    "- Basic Arithmetic\n",
    "- Fractions\n",
    "- Introductory Algebra\n",
    "- Basic Scientific Principles (biology, chemistry, physics)\n",
    "- World History\n",
    "- Government Systems\n",
    "- Citizenship Rights and Responsibilities\n",
    "- Fitness\n",
    "- Self-Expression\n",
    "- Digital Literacy\n",
    "\n",
    "Subjects:\n",
    "- Language Arts, Mathematics, Science, History, Social Studies, Physical Education, Arts, Computer Literacy\n",
    "\n",
    "\n",
    "\n",
    "Use the variables already defined above, `model_gpro` to generate the relevant content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns.\n",
    "\n",
    "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model_gpro.start_chat()\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can send another message to continue the conversation. The previous conversation is automatically sent in the next message as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What are the main components of a computer?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the system instruction\n",
    "\n",
    "The system instruction in Gemini is a tool for developers to fine-tune the model's responses for specific tasks. It lets them define various aspects of how Gemini should generate responses [2].\n",
    "\n",
    "Here are some key benefits of system instructions:\n",
    "\n",
    "**Role definition:** You can specify the role Gemini should play, such as a home-cooking assistant or a music historian.\n",
    "\n",
    "**Format control:** Instruct Gemini on the format of the response, like text, a list, or even a structured JSON object.\n",
    "\n",
    "**Goal setting:** Clearly define the goal you want Gemini to achieve, making the response more focused and relevant.\n",
    "\n",
    "**Rule establishment:** Set rules for Gemini to follow, ensuring the response adheres to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gprosys = genai.GenerativeModel(\n",
    "    MODEL,\n",
    "    system_instruction=\"You are a primary school teacher specializing in early childhood education. Use positive reinforcement and interactive methods to teach basic concepts. Adapt your responses to the learning style of a young child.\",\n",
    ")\n",
    "\n",
    "response = model_gprosys.generate_content(\"Share a story for children on kindness.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Do it yourself_\n",
    "`10 mins`\n",
    "\n",
    "Create a simple chatbot designed to help middle school students learn more about our moon. (i.e. children learn about the moon by chatting with it)\n",
    "\n",
    "Before we start writing code, lets think about the problem first. We want to achieve a specific goal when the child uses this chatbot. \n",
    "\n",
    "1. What are some Student Learning objectives we might want to cover during the conversations\n",
    "2. How do you assess the child had learnt something from the chatbot?\n",
    "3. How do we control the language so it can be easily understood by children.\n",
    "\n",
    "Once we have a plan of action - lets create the chatbot itself.\n",
    "\n",
    "1. Setup a chat session.\n",
    "2. Set the system instruction. Think about the questions above we have asked.\n",
    "3. Think about the character and safeguards for children. (Configure the safety settings: https://ai.google.dev/gemini-api/docs/safety-settings#request-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Multimodality\n",
    "\n",
    "We have used images already - one aspect of multi-modal is audio. Lets try that out as well.\n",
    "\n",
    "We will download the audio first, and then use it in our prompt.\n",
    "\n",
    "Incase of audio - Gemini also understands Urdu! You can try an audio from here to test:\n",
    "\n",
    "https://github.com/siddiquelatif/URDU-Dataset/tree/master/Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"\n",
    "!wget -q $URL -O sample.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_file = genai.upload_file(path='sample.mp3')\n",
    "prompt = \"Listen carefully to the following audio file. Provide a brief summary.\"\n",
    "response = model_gpro.generate_content([prompt, your_file])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "`10 mins`\n",
    "\n",
    "Let's also try doing the same using some PDFs!\n",
    "\n",
    "1. Download the PDF file from here: https://assets.openstax.org/oscms-prodcms/media/documents/UniversityPhysicsVolume2-WEB_5eNhMSa.pdf\n",
    "\n",
    "2. This is a Physics book. Lets extract the 3rd chapter (pages [121-154]) from this in form of images using `pdftoppm` or any other library.\n",
    "\n",
    "3. Use `pdftotext` or alternative to perform OCR on the images. (Can Gemini perform the OCR for us?)\n",
    "\n",
    "4. Concat all the text and send to Gemini to generate a summary for the chapter.\n",
    "\n",
    "5. Also generate an assessment for the student to review if they have understood the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling\n",
    "\n",
    "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
    "\n",
    "> Important: The SDK converts function parameter type annotations to a format the API understands (`glm.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:float, b:float):\n",
    "    \"\"\"returns a + b.\"\"\"\n",
    "    return a+b\n",
    "\n",
    "def subtract(a:float, b:float):\n",
    "    \"\"\"returns a - b.\"\"\"\n",
    "    return a-b\n",
    "\n",
    "def multiply(a:float, b:float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:float, b:float):\n",
    "    \"\"\"returns a / b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "model_gprofunc = genai.GenerativeModel(model_name=MODEL,\n",
    "                              tools=[add, subtract, multiply, divide])\n",
    "\n",
    "chat = model_gprofunc.start_chat(enable_automatic_function_calling=True)\n",
    "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
    "\n",
    "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a [`glm.Content`](https://ai.google.dev/api/python/google/ai/generativelanguage/Content) object, which contains the following information:\n",
    "\n",
    "*   **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
    "*   **Parts**: A list of [`glm.Part`](https://ai.google.dev/api/python/google/ai/generativelanguage/Part) objects that represent individual components of the message. With a text-only model, these parts can be:\n",
    "    *   **Text**: Plain text messages.\n",
    "    *   **Function Call** ([`glm.FunctionCall`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall)): A request from the model to execute a specific function with provided arguments.\n",
    "    *   **Function Response** ([`glm.FunctionResponse`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse)): The result returned by the user after executing the requested function.\n",
    "\n",
    " In the previous example with the mittens calculation, the history shows the following sequence:\n",
    "\n",
    "1.  **User**: Asks the question about the total number of mittens.\n",
    "1.  **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
    "1.  **User**: The `ChatSession` automatically executes the function (due to `enable_automatic_function_calling` being set) and sends back a `FunctionResponse` with the calculated result.\n",
    "1.  **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in chat.history:\n",
    "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it your self\n",
    "\n",
    "`15 mins`\n",
    "\n",
    "Create a script below which will use function calling to fetch the latest weather when asked about for a specific location.\n",
    "\n",
    "Bonus: Ask Gemini to write the code for you!\n",
    "\n",
    "Ask Gemini for a weather API to use. And then configure it was a function as described above.\n",
    "\n",
    "An example prompt for it can be: `Whats the weather like in Karachi today?`\n",
    "\n",
    "You can use the free api available on: https://www.weatherapi.com/\n",
    "\n",
    "Bonus: Configure the temperature unit as well (C or F) as per prompt, with C being default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Caching\n",
    "\n",
    "One of the cool new features which has been added by Gemini is prompt caching. If a part of your prompt or instructions is not changing, you can save some tokens by caching that part of the prompt. This is very useful in production where the same prompt might be used for thousands and millions of times and allows us to optimize cost.\n",
    "\n",
    "In order to use the cache, we create a cache context and then call the generate content endpoint with the additional context.\n",
    "\n",
    "First we download some data which is reused again and again across context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
    "!head a11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# upload the files.\n",
    "document = genai.upload_file(path=\"a11.txt\")\n",
    "\n",
    "# create the caching context\n",
    "apollo_cache = caching.CachedContent.create(\n",
    "    model=MODEL,\n",
    "    system_instruction=\"You are an expert at analyzing transcripts.\",\n",
    "    contents=[document],\n",
    ")\n",
    "\n",
    "# update the expiry on the cache\n",
    "apollo_cache.update(ttl=datetime.timedelta(hours=2))\n",
    "\n",
    "# use the cache to create the model\n",
    "apollo_model = genai.GenerativeModel.from_cached_content(cached_content=apollo_cache)\n",
    "\n",
    "# use the model to generate the response.\n",
    "response = apollo_model.generate_content(\"Find a lighthearted moment from this transcript\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have used the cache you can also delete it (or it expires automatically at the given time)\n",
    "apollo_cache.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it your self!\n",
    "\n",
    "`10 mins`\n",
    "\n",
    "Create a cache context using the images from this link:\n",
    "\n",
    "https://storage.googleapis.com/generativeai-downloads/data/clothes-dataset.zip\n",
    "\n",
    "And then ask different questions from it to see if the cache is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus round!\n",
    "\n",
    "### Building a Simple Story Generator with Gemini Chat API\n",
    "\n",
    "**Problem statement**\n",
    "\n",
    "When kids are learning how to read - for every kid its a very difficult and slow process. The only way to get better is practice. However the practice needs to be at the right level, simply reading the same book a thousand times isnt good enough. \n",
    "\n",
    "Enter our story generator. \n",
    "1. It can be designed to generate content at the right level, so for example grade one. \n",
    "2. It can generate new content every time\n",
    "3. It can generate content which appeals to the child, where they are the hero (check link below).\n",
    "\n",
    "The right product will help the kids learn to read better over time.\n",
    "\n",
    "**Theme Selection**\n",
    "\n",
    "Present a list of themes (e.g., fantasy, sci-fi, mystery, historical) when the session starts.\n",
    "Allow the user to input a theme or select from the list.\n",
    "\n",
    "**Initial Story Generation**\n",
    "\n",
    "Based on the selected theme, generate a short paragraph introducing the story and the user's character. Ensure its safe for children, uses easy to use language and as creative as possible.\n",
    "\n",
    "**Action Selection**\n",
    "\n",
    "Provide multiple action choices related to the current story.\n",
    "Allow the user to select an action.\n",
    "\n",
    "**Story Continuation**\n",
    "\n",
    "Generate a new paragraph based on the user's chosen action, advancing the story.\n",
    "Repeat steps 3 and 4 until a desired story length or ending condition is reached.\n",
    "\n",
    "Idea is inspired by https://www.wander.ly/ - check their website for more inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you.\n",
    "\n",
    "Thank you for attending this workshop. You can find more details about me on https://karachiwala.dev. I am available over most platforms as @mashhoodr.\n",
    "\n",
    "You can find many more examples for Gemini on the following repositories.\n",
    "\n",
    "- https://github.com/google-gemini/cookbook\n",
    "- https://github.com/GoogleCloudPlatform/generative-ai\n",
    "\n",
    "If you have any feedback on this workshop please share it with me using the following link:\n",
    "\n",
    "https://docs.google.com/forms/d/1iAEO1JSlh6GTLC0uudUxAiTDiNN_iMzfdCwDLZ_78sg/edit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
