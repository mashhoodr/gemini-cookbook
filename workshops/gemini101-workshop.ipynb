{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini BuildWithAI Workshop\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mashhoodr/gemini-cookbook/blob/main/workshops/gemini101-workshop.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "\n",
    "\n",
    "This notebook is designed to run you through different features of Google Gemini. Please follow the instructions of the trainer. It has content taken from different cookbook files, aggregated for convenience. \n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "The objective of this workshop is to help the attendees become familiar with the offerings of Google Gemini, and give them an opportunity to try out the API themselves. We run through a few exercises to help understand the use cases for the different functionalities present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Gemini API uses API keys for authentication. We will now setup the API key in this colab - and test out our authentication. Your trainer has already demoed the instructions below.\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.  \n",
    "\n",
    "Remember to treat your API key like a password. Do not accidentally save it in a notebook or source file you later commit to GitHub. This notebook shows you two ways you can securely store your API key.\n",
    "\n",
    "* If you are using Google Colab, we recommend you store your key in Colab Secrets.\n",
    "\n",
    "* If you are using a different development environment (or calling the Gemini API through `cURL` in your terminal), we recommend you store your key in an environment variable.\n",
    "\n",
    "Let's start with Colab Secrets.\n",
    "\n",
    "Add your API key to the Colab Secrets manager to securely store it.\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ **Secrets** tab in the left panel.\n",
    "   \n",
    "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
    "\n",
    "2. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "3. Copy/paste your API key into the `Value` input box of `GOOGLE_API_KEY`.\n",
    "4. Toggle the button on the left to allow notebook access to the secret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the SDK with your API key.\n",
    "\n",
    "You'll call `genai.configure` with your API key, but instead of pasting your key into the notebook, you'll read it from Colab Secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now you're ready to use the Gemini API.\n",
    "\n",
    "Now lets list our all the models we have available to use, before we continue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running your first prompt\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpro = genai.GenerativeModel('gemini-1.0-pro')\n",
    "response = model_gpro.generate_content(\"Write a short poem on Python programming language.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do it yourself: Update the above using the latest Gemini version available*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use images in your prompt\n",
    "\n",
    "Here we download an image from a URL and pass that image in our prompt.\n",
    "\n",
    "First, we download the image and load it with PIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o image.jpg https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "img = PIL.Image.open('image.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"This image contains a sketch of a potential product along with some notes.\n",
    "Given the product sketch, describe the product as thoroughly as possible based on what you\n",
    "see in the image, making sure to note all of the product features. Return output in json format:\n",
    "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can include the image in our prompt by just passing a list of items to `generate_content`. Note that you will need to use the `gemini-pro-vision` model if your prompt contains images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gprov = genai.GenerativeModel('gemini-pro-vision')\n",
    "response = model_gprov.generate_content([prompt, img])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Prompt Engineering\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "We have shared several important examples below, if you are interested in diving into more details, please checkout: https://www.promptingguide.ai/\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt.\n",
    "e.g. \n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety\n",
    "\n",
    "Creating good prompts needs some thought and structure, the following points should be considered when generating a good prompt.\n",
    "\n",
    "1. Define the task to perform. e.g. Summarize this text.\n",
    "2. Specify any constraints e.g. Summarize this text in two sentences.\n",
    "3. Define the format of the response e.g. Summarize this text as bullets points of key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different prompt instructions from above.\n",
    "prompt = \"\"\"\n",
    "Summarize this text as bullets points of key information.\n",
    "Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\n",
    "faster than any modern traditional computer. At very tiny scales, physical matter acts as both\n",
    "particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\n",
    "The operating principles of quantum devices is beyond the scope of classical physics. When deployed\n",
    "at scale, quantum computers could be used in a wide variety of applications such as: in\n",
    "cybersecurity to break existing encryption methods while helping researchers create new ones, in\n",
    "meteorology to develop better weather forecasting etc. However, the current state of the art quantum\n",
    "computers are still largely experimental and impractical.\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do it yourself: Try summarizing the above paragraph using different instructions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Include few-shot examples. \n",
    "\n",
    "You can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called few-shot prompts, while prompts that provide no examples are called zero-shot prompts. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Instructions: Tell me the subject that each lesson topic belongs to.\n",
    "\n",
    "Lesson Topic: The Life Cycle of a Butterfly -> Subject: Science\n",
    "Lesson Topic: Using Commas -> Subject: Language Arts (Grammar)\n",
    "Lesson Topic: Solving Equations with X -> Subject: Math\n",
    "Your Turn:\n",
    "Lesson Topic: The Different Parts of Speech -> Subject: _____\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Chain of thought prompts\n",
    "\n",
    "Chain-of-Thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
    "\n",
    "<img src=\"https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1080&q=75\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
    "\n",
    "So there are 2 different ways to manage the prompt complexity - one is to show the working and the other is to ask it to think it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Prompt the model to format its response (i.e. json mode)\n",
    "\n",
    "To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of 10 countries with their capitals in the following json format:\n",
    "{country: country_name, capital: capital_name}\n",
    "\"\"\"\n",
    "\n",
    "response = model_gpro.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch out for hallucinations!\n",
    "\n",
    "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isnâ€™t a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Do it yourself._\n",
    "`10 mins`\n",
    "\n",
    "1. Generate some Python tips for a newsletter. How can you make a good prompt to deliver unique tips on multiple attempts?\n",
    "2. Use the following image, ask Gemin to solve the puzzle and explain it step by step. https://i.ibb.co/68ww1v8/Screenshot-2024-04-12-at-4-57-14-PM.png\n",
    "3. Generate a SQL query using Gemini, from a table `Countries`, with columns `CountryName` and `CapitalName`, to select all those countries whose capital starts with `M`. \n",
    "\n",
    "Bonus: How can we test this SQL query within Gemini? (hint: generate the testing data using Gemini too)\n",
    "\n",
    "Use the variables already defined above, `model_gpro` and `model_gprov` to generate the relevant content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns.\n",
    "\n",
    "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model_gpro.start_chat()\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can send another message to continue the conversation. The previous conversation is automatically sent in the next message as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What are the main components of a computer?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the system instruction\n",
    "\n",
    "The system instruction in Gemini is a tool for developers to fine-tune the model's responses for specific tasks. It lets them define various aspects of how Gemini should generate responses [2].\n",
    "\n",
    "Here are some key benefits of system instructions:\n",
    "\n",
    "**Role definition:** You can specify the role Gemini should play, such as a home-cooking assistant or a music historian.\n",
    "\n",
    "**Format control:** Instruct Gemini on the format of the response, like text, a list, or even a structured JSON object.\n",
    "\n",
    "**Goal setting:** Clearly define the goal you want Gemini to achieve, making the response more focused and relevant.\n",
    "\n",
    "**Rule establishment:** Set rules for Gemini to follow, ensuring the response adheres to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gprosys = genai.GenerativeModel(\n",
    "    \"gemini-1.5-pro-latest\",\n",
    "    system_instruction=\"You are a cat. Your name is Neko.\",\n",
    ")\n",
    "\n",
    "response = model_gprosys.generate_content(\"Good morning! How are you?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the temperature and stop sequence\n",
    "\n",
    "Every prompt you send to the model includes parameters that control how the model generates responses. Use a `genai.GenerationConfig` to set these, or omit it to use the defaults.\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Use higher values for more creative responses, and lower values for more deterministic responses.\n",
    "\n",
    "You can set the `generation_config` when creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gprotemp = genai.GenerativeModel(\n",
    "  \"gemini-1.0-pro\",\n",
    "  generation_config=genai.GenerationConfig(\n",
    "      max_output_tokens=2000,\n",
    "      temperature=0.9,\n",
    "  )\n",
    ")\n",
    "\n",
    "response = model_gprotemp.generate_content(\n",
    "    'Give me a numbered list of cat facts.',\n",
    "    # Limit to 5 facts.\n",
    "    generation_config = genai.GenerationConfig(stop_sequences=['\\n6'])\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Do it yourself_\n",
    "`10 mins`\n",
    "\n",
    "Create a simple chatbot designed to help middle school students learn more about our moon. (i.e. children learn about the moon by chatting with it)\n",
    "\n",
    "1. Setup a chat session.\n",
    "2. Set the system instruction. Think about the character and safeguards for children. (Configure the safety settings: https://ai.google.dev/gemini-api/docs/safety-settings#request-example)\n",
    "3. Play with temperature to see some difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Multimodality\n",
    "\n",
    "We have used images already - one aspect of multi-modal is audio. Lets try that out as well.\n",
    "\n",
    "We will download the audio first, and then use it in our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"\n",
    "!wget -q $URL -O sample.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_file = genai.upload_file(path='sample.mp3')\n",
    "prompt = \"Listen carefully to the following audio file. Provide a brief summary.\"\n",
    "response = model_gpro.generate_content([prompt, your_file])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it your self\n",
    "\n",
    "`10 mins`\n",
    "\n",
    "1. Record some audio on your laptop or download something from the internet and test to see if the Gemini API understands Urdu audio.\n",
    "\n",
    "Upload the file into Colab from your machine first and then use it below.\n",
    "\n",
    "Alternatively you can also use audios from here: \n",
    "https://github.com/siddiquelatif/URDU-Dataset/tree/master/Neutral\n",
    "\n",
    "2. Download a short sample video, and ask a question on it using the same code as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling\n",
    "\n",
    "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
    "\n",
    "> Important: The SDK converts function parameter type annotations to a format the API understands (`glm.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:float, b:float):\n",
    "    \"\"\"returns a + b.\"\"\"\n",
    "    return a+b\n",
    "\n",
    "def subtract(a:float, b:float):\n",
    "    \"\"\"returns a - b.\"\"\"\n",
    "    return a-b\n",
    "\n",
    "def multiply(a:float, b:float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:float, b:float):\n",
    "    \"\"\"returns a / b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "model_gprofunc = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
    "                              tools=[add, subtract, multiply, divide])\n",
    "\n",
    "chat = model_gprofunc.start_chat(enable_automatic_function_calling=True)\n",
    "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
    "\n",
    "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a [`glm.Content`](https://ai.google.dev/api/python/google/ai/generativelanguage/Content) object, which contains the following information:\n",
    "\n",
    "*   **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
    "*   **Parts**: A list of [`glm.Part`](https://ai.google.dev/api/python/google/ai/generativelanguage/Part) objects that represent individual components of the message. With a text-only model, these parts can be:\n",
    "    *   **Text**: Plain text messages.\n",
    "    *   **Function Call** ([`glm.FunctionCall`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall)): A request from the model to execute a specific function with provided arguments.\n",
    "    *   **Function Response** ([`glm.FunctionResponse`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse)): The result returned by the user after executing the requested function.\n",
    "\n",
    " In the previous example with the mittens calculation, the history shows the following sequence:\n",
    "\n",
    "1.  **User**: Asks the question about the total number of mittens.\n",
    "1.  **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
    "1.  **User**: The `ChatSession` automatically executes the function (due to `enable_automatic_function_calling` being set) and sends back a `FunctionResponse` with the calculated result.\n",
    "1.  **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in chat.history:\n",
    "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it your self\n",
    "\n",
    "`15 mins`\n",
    "\n",
    "Create a script below which will use function calling to fetch the latest weather when asked about for a specific location.\n",
    "\n",
    "Bonus: Ask Gemini to write the code for you!\n",
    "\n",
    "Ask Gemini for a weather API to use. And then configure it was a function as described above.\n",
    "\n",
    "An example prompt for it can be: `Whats the weather like in Karachi today?`\n",
    "\n",
    "You can use the free api available on: https://www.weatherapi.com/\n",
    "\n",
    "Bonus: Configure the temperature unit as well (C or F) as per prompt, with C being default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you.\n",
    "\n",
    "Thank you for attending this workshop. You can find more details about me on https://karachiwala.dev. I am available over most platforms as @mashhoodr.\n",
    "\n",
    "You can find many more examples for Gemini on the following repositories.\n",
    "\n",
    "- https://github.com/google-gemini/cookbook\n",
    "- https://github.com/GoogleCloudPlatform/generative-ai\n",
    "\n",
    "If you have any feedback on this workshop please share it with me using the following link:\n",
    "\n",
    "https://docs.google.com/forms/d/1iAEO1JSlh6GTLC0uudUxAiTDiNN_iMzfdCwDLZ_78sg/edit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
