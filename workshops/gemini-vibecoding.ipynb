{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini BuildWithAI 2025 Workshop\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mashhoodr/gemini-cookbook/blob/main/workshops/gemini-vibecoding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "\n",
    "\n",
    "This notebook is designed to help you become familiar with Gemini's API while creating some simple projects. We use \"vibe coding\" to help build some basic apps and test out the power of Gemini.\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "The objective of this workshop is to help the attendees become familiar with the offerings of Google Gemini, and give them an opportunity to try out the API themselves. We run through a few exercises to help understand the use cases for the different functionalities present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Gemini API uses API keys for authentication. We will now setup the API key in this colab - and test out our authentication. Your trainer has already demoed the instructions below.\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.  \n",
    "\n",
    "Remember to treat your API key like a password. Do not accidentally save it in a notebook or source file you later commit to GitHub. This notebook shows you two ways you can securely store your API key.\n",
    "\n",
    "* If you are using Google Colab, we recommend you store your key in Colab Secrets.\n",
    "\n",
    "* If you are using a different development environment (or calling the Gemini API through `cURL` in your terminal), we recommend you store your key in an environment variable.\n",
    "\n",
    "Let's start with Colab Secrets.\n",
    "\n",
    "Add your API key to the Colab Secrets manager to securely store it.\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ **Secrets** tab in the left panel.\n",
    "   \n",
    "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
    "\n",
    "2. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "3. Copy/paste your API key into the `Value` input box of `GOOGLE_API_KEY`.\n",
    "4. Toggle the button on the left to allow notebook access to the secret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the SDK with your API key.\n",
    "\n",
    "You'll call `genai.configure` with your API key, but instead of pasting your key into the notebook, you'll read it from Colab Secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.colab import userdata\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import Markdown, HTML, Image\n",
    "\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now you're ready to use the Gemini API.\n",
    "\n",
    "Now lets set our model we want to use throughout this notebook. You can change this to any available model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running your first prompt\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL, contents=\"Explain how AI works in a few words.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Do it yourself: Update the above using a different Gemini version available. Did the response change?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use images in your prompt\n",
    "\n",
    "Here we download an image from a URL and pass that image in our prompt.\n",
    "\n",
    "First, we download the image and load it with PIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o image.jpg https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "img = PIL.Image.open('image.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"This image contains a sketch of a potential product along with some notes.\n",
    "Given the product sketch, describe the product as thoroughly as possible based on what you\n",
    "see in the image, making sure to note all of the product features. Return output in json format:\n",
    "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can include the image in our prompt by just passing a list of items to `generate_content`. You can pass in multiple images, or prompts or files as per your requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=[prompt, img]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading files\n",
    "\n",
    "For types other than image, like audio, video or pdf - you can use the `upload_file` function to send data to Gemini.\n",
    "\n",
    "The following list of documents are supported:\n",
    "\n",
    "- PDF - application/pdf\n",
    "- JavaScript - application/x-javascript, text/javascript\n",
    "- Python - application/x-python, text/x-python\n",
    "- TXT - text/plain\n",
    "- HTML - text/html\n",
    "- CSS - text/css\n",
    "- Markdown - text/md\n",
    "- CSV - text/csv\n",
    "- XML - text/xml\n",
    "- RTF - text/rtf\n",
    "\n",
    "First download a PDF file into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\"\n",
    "!wget -q $URL -O sample.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass it into Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import io\n",
    "import httpx\n",
    "\n",
    "\n",
    "your_file = client.files.upload(file='sample.pdf')\n",
    "prompt = \"Can you summarize this file as a bulleted list?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model=MODEL,\n",
    "  contents=[your_file, prompt])\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply a schema through model configuration\n",
    "The following example does the following:\n",
    "\n",
    "Instantiates a model configured through a schema to respond with JSON.\n",
    "Prompts the model to return cookie recipes.\n",
    "\n",
    "The Gemini API Python client library supports schemas defined with the following types (where AllowedType is any allowed type):\n",
    "\n",
    "- int\n",
    "- float\n",
    "- bool\n",
    "- str\n",
    "- list[AllowedType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Recipe(BaseModel):\n",
    "  recipe_name: str\n",
    "  ingredients: list[str]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents='List a few popular cookie recipes.',\n",
    "    config={\n",
    "        'response_mime_type': 'application/json',\n",
    "        'response_schema': list[Recipe],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Use the response as a JSON string.\n",
    "print(response.text)\n",
    "\n",
    "# Use instantiated objects.\n",
    "my_recipes: list[Recipe] = response.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use an enum to constrain output\n",
    "In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an enum in your schema. You can use an enum option anywhere you could use a str in the response_schema, because an enum is a list of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.\n",
    "\n",
    "For example, assume that you're developing an application to classify musical instruments into one of five categories: \"Percussion\", \"String\", \"Woodwind\", \"Brass\", or \"\"Keyboard\"\". You could create an enum to help with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Instrument(enum.Enum):\n",
    "  PERCUSSION = \"Percussion\"\n",
    "  STRING = \"String\"\n",
    "  WOODWIND = \"Woodwind\"\n",
    "  BRASS = \"Brass\"\n",
    "  KEYBOARD = \"Keyboard\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents='What type of instrument is an oboe?',\n",
    "    config={\n",
    "        'response_mime_type': 'text/x.enum',\n",
    "        'response_schema': Instrument,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns.\n",
    "\n",
    "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model=MODEL)\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can send another message to continue the conversation. The previous conversation is automatically sent in the next message as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What are the main components of a computer?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the system instruction\n",
    "\n",
    "The system instruction in Gemini is a tool for developers to fine-tune the model's responses for specific tasks. It lets them define various aspects of how Gemini should generate responses [2].\n",
    "\n",
    "Here are some key benefits of system instructions:\n",
    "\n",
    "**Role definition:** You can specify the role Gemini should play, such as a home-cooking assistant or a music historian.\n",
    "\n",
    "**Format control:** Instruct Gemini on the format of the response, like text, a list, or even a structured JSON object.\n",
    "\n",
    "**Goal setting:** Clearly define the goal you want Gemini to achieve, making the response more focused and relevant.\n",
    "\n",
    "**Rule establishment:** Set rules for Gemini to follow, ensuring the response adheres to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=[\"Share a short story for children on kindness.\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1,\n",
    "        system_instruction=\"You are a primary school teacher specializing in early childhood education. Use positive reinforcement and interactive methods to teach basic concepts. Adapt your responses to the learning style of a young child.\"\n",
    "    )\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some practice!\n",
    "\n",
    "In the world of AI Agents and Coding tools from LLMs, there is a new way to learn. Instead of reading API documentations and creating demos by hand, we want to use the power of LLMs to quickly build the scaffolding for us. This allows us to play with the technology at a much deeper level, rather than spending hours before just \"setting up\" some basic things.\n",
    "\n",
    "For our workshop today, we will be using the recently launched **Firebase Studio**. Access it on https://firebase.studio/ and lets create the first workspace for the exercise below. \n",
    "\n",
    "If for any reason Firebase Studio isnt working, then I would recommend https://lovable.dev/\n",
    "\n",
    "Alternatively those comfortable using Cursor, Copilot, Replit or Windsurf - feel free to use that. I would recommend using the Gemini Pro 2.5 API with them for a better Gemini SDK integration experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: A Simple Story Generator with Gemini Chat API\n",
    "\n",
    "**Problem statement**\n",
    "\n",
    "When kids are learning how to read - for every kid its a very difficult and slow process. The only way to get better is practice. However the practice needs to be at the right level, simply reading the same book a thousand times isnt good enough. \n",
    "\n",
    "Enter our story generator. \n",
    "1. It can be designed to generate content at the right level, so for example grade one. \n",
    "2. It can generate new content every time\n",
    "3. It can generate content which appeals to the child, where they are the hero (check link below).\n",
    "\n",
    "The right product will help the kids learn to read better over time.\n",
    "\n",
    "**Theme Selection**\n",
    "\n",
    "Present a list of themes (e.g., fantasy, sci-fi, mystery, historical) when the session starts.\n",
    "Allow the user to input a theme or select from the list.\n",
    "\n",
    "**Initial Story Generation**\n",
    "\n",
    "Based on the selected theme, generate a short paragraph introducing the story and the user's character. Ensure its safe for children, uses easy to use language and as creative as possible.\n",
    "\n",
    "**Action Selection**\n",
    "\n",
    "Provide multiple action choices related to the current story.\n",
    "Allow the user to select an action.\n",
    "\n",
    "**Story Continuation**\n",
    "\n",
    "Generate a new paragraph based on the user's chosen action, advancing the story.\n",
    "Repeat steps 3 and 4 until a desired story length or ending condition is reached.\n",
    "\n",
    "Idea is inspired by https://www.wander.ly/ - check their website for more inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's continue to dive deeper with Function calling\n",
    "\n",
    "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
    "\n",
    "> Important: The SDK converts function parameter type annotations to a format the API understands (`glm.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:float, b:float):\n",
    "    \"\"\"returns a + b.\"\"\"\n",
    "    return a+b\n",
    "\n",
    "def subtract(a:float, b:float):\n",
    "    \"\"\"returns a - b.\"\"\"\n",
    "    return a-b\n",
    "\n",
    "def multiply(a:float, b:float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:float, b:float):\n",
    "    \"\"\"returns a / b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"tools\": [add, subtract, multiply, divide],\n",
    "}\n",
    "chat = client.chats.create(model=MODEL, config=config)\n",
    "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
    "\n",
    "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a [`glm.Content`](https://ai.google.dev/api/python/google/ai/generativelanguage/Content) object, which contains the following information:\n",
    "\n",
    "*   **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
    "*   **Parts**: A list of [`glm.Part`](https://ai.google.dev/api/python/google/ai/generativelanguage/Part) objects that represent individual components of the message. With a text-only model, these parts can be:\n",
    "    *   **Text**: Plain text messages.\n",
    "    *   **Function Call** ([`glm.FunctionCall`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall)): A request from the model to execute a specific function with provided arguments.\n",
    "    *   **Function Response** ([`glm.FunctionResponse`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse)): The result returned by the user after executing the requested function.\n",
    "\n",
    " In the previous example with the mittens calculation, the history shows the following sequence:\n",
    "\n",
    "1.  **User**: Asks the question about the total number of mittens.\n",
    "1.  **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
    "1.  **User**: The `ChatSession` automatically executes the function (due to `enable_automatic_function_calling` being set) and sends back a `FunctionResponse` with the calculated result.\n",
    "1.  **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in chat.get_history():\n",
    "    print(content.role, \"->\", [(str(part.function_call) + ' -> ' + str(part.function_response)) for part in content.parts])\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Model Context Protocol (MCP)\n",
    "\n",
    "Model Context Protocol (MCP) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n",
    "\n",
    "MCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. You can learn more about MCP and how to use it in the documentation: \n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/function-calling?example=weather#use_model_context_protocol_mcp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Code Execution \n",
    "\n",
    "The Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The code execution environment includes the following libraries: altair, chess, cv2, matplotlib, mpmath, numpy, pandas, pdfminer, reportlab, seaborn, sklearn, statsmodels, striprtf, sympy, and tabulate. You can't install your own libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "  model=MODEL,\n",
    "  contents='What is the sum of the first 50 prime numbers? '\n",
    "           'Generate and run code for the calculation, and make sure you get all 50.',\n",
    "  config=types.GenerateContentConfig(\n",
    "    tools=[types.Tool(\n",
    "      code_execution=types.ToolCodeExecution\n",
    "    )]\n",
    "  )\n",
    ")\n",
    "\n",
    "def display_code_execution_result(response):\n",
    "  for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "      display(Markdown(part.text))\n",
    "    if part.executable_code is not None:\n",
    "      code_html = f'<pre style=\"background-color: #BBBBEE;\">{part.executable_code.code}</pre>' # Change code color\n",
    "      display(HTML(code_html))\n",
    "    if part.code_execution_result is not None:\n",
    "      display(Markdown(part.code_execution_result.output))\n",
    "    if part.inline_data is not None:\n",
    "      display(Image(data=part.inline_data.data, format=\"png\"))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "display_code_execution_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Caching\n",
    "\n",
    "One of the cool new features which has been added by Gemini is prompt caching. If a part of your prompt or instructions is not changing, you can save some tokens by caching that part of the prompt. This is very useful in production where the same prompt might be used for thousands and millions of times and allows us to optimize cost.\n",
    "\n",
    "In order to use the cache, we create a cache context and then call the generate content endpoint with the additional context.\n",
    "\n",
    "First we download some data which is reused again and again across context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
    "!head a11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# upload the files.\n",
    "document = client.files.upload(file=\"a11.txt\")\n",
    "\n",
    "cache_model='models/gemini-2.0-flash-001'\n",
    "\n",
    "# Create a cache with a 5 minute TTL\n",
    "cache = client.caches.create(\n",
    "    model=cache_model,\n",
    "    config=types.CreateCachedContentConfig(\n",
    "      display_name='transcript',\n",
    "      system_instruction=(\n",
    "          \"You are an expert at analyzing transcripts.\"\n",
    "      ),\n",
    "      contents=[document],\n",
    "      ttl=\"300s\",\n",
    "  )\n",
    ")\n",
    "\n",
    "# Construct a GenerativeModel which uses the created cache.\n",
    "response = client.models.generate_content(\n",
    "  model = cache_model,\n",
    "  contents= (\"Find a lighthearted moment from this transcript\"),\n",
    "  config=types.GenerateContentConfig(cached_content=cache.name)\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have used the cache you can also delete it (or it expires automatically at the given time)\n",
    "client.caches.delete(name=cache.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: A Weather App using Function Calling.\n",
    "\n",
    "Create a simple weather app using Firebase Studio which will use function calling to fetch the latest weather when asked about for a specific location. \n",
    "\n",
    "An example prompt for it can be: `Whats the weather like in Islamabad today?`\n",
    "\n",
    "You can use the free api available on: https://www.weatherapi.com/\n",
    "\n",
    "Bonus: Configure the temperature unit toggle as well (C or F), with C being default.\n",
    "\n",
    "When you ask for `Hows the weather in Karachi this week?` is it able to adjust its UI to show the result?\n",
    "\n",
    "What happens when you ask it a question like `Do I need an umbrella in Islamabad today?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: An offline-first Learning Management System (LMS)\n",
    "\n",
    "This is a slightly more complex one, we definitely dont want to single shot this app. I have listed down some prompts/requirements below - you can use these to build up the app step by step. You can choose to change these prompts or add any other functionality you like as well. The focus here is on integrating Gemini to generate courses, lessons and assessments. \n",
    "\n",
    "As you build up the functionality, think about testing this app, and how you will ensure that nothing is breaking after each iteration.\n",
    "\n",
    "1. \"I want to build a personal learning management system, similar to Udemy where I can create courses and track my progress in them over time. It should all work offline on the client using localstorage.\"\n",
    "2. It should contain a structure having courses, sections, lessons first. We will add assessments later.\n",
    "3. You should be able to add, edit or delete courses, sections and lessons. They should persist on refreshing the page.\n",
    "3. Ability to add Youtube links in Lessons.\n",
    "4. Ability to manually add assessments (basic mcqs) in the Lessons.\n",
    "5. Move everything to IndexedDB. (refactoring ability)\n",
    "6. Feature to generate Sections and Lessons using Gemini.\n",
    "7. Feature to generate assessments using Gemini.\n",
    "8. Use the caching feature to upload a large PDF or video and use that to create multiple lessons and assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you.\n",
    "\n",
    "Thank you for attending this workshop. You can find more details about me on https://karachiwala.dev. I am available over most platforms as @mashhoodr.\n",
    "\n",
    "You can find many more examples for Gemini on the following repositories.\n",
    "\n",
    "- https://github.com/google-gemini/cookbook\n",
    "- https://github.com/GoogleCloudPlatform/generative-ai\n",
    "\n",
    "If you have any feedback on this workshop please share it with me using the following link:\n",
    "\n",
    "https://docs.google.com/forms/d/1iAEO1JSlh6GTLC0uudUxAiTDiNN_iMzfdCwDLZ_78sg/edit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
